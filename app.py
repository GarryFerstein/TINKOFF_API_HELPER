# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

import streamlit as st
import requests
import os
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from litellm import completion
import tiktoken
from dotenv import load_dotenv
import multiprocessing as mp

# === –û–¢–ö–õ–Æ–ß–ê–ï–ú –ú–£–õ–¨–¢–ò–ü–†–û–¶–ï–°–°–ò–ù–ì –î–õ–Ø FAISS –ò TRANSFORMERS ===
# –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º segmentation fault –∏ —É—Ç–µ—á–∫–∏ —Å–µ–º–∞—Ñ–æ—Ä–æ–≤
mp.set_start_method('spawn', force=True)
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º HuggingFace
os.environ["OMP_NUM_THREADS"] = "1"  # –û—Ç–∫–ª—é—á–∞–µ–º OpenMP –≤ NumPy/FAISS

# === –ó–∞–≥—Ä—É–∑–∫–∞ .env ===
load_dotenv()

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
DOC_ID = "1T_X6a4uRjPLvsHnYKsCwBda77RfHxJmgFxtQsbtUFq8"
INDEX_PATH = "tinkoff_faiss.index"
CHUNKS_PATH = "chunks.npy"
EMBEDDER_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
MAX_TOKENS = 12000
ENCODING = tiktoken.encoding_for_model("gpt-4o-mini")

# === –ü—Ä–æ–º–ø—Ç ===
PROMPT_TEMPLATE = """–¢—ã ‚Äî Tinkoff API Helper, —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Tinkoff Invest API.
–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É `tinkoff-invest-api`.

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π –∫–æ–¥–æ–º –Ω–∞ Python, —Å–æ–≤–µ—Ç–æ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π.
- –ò—Å–ø–æ–ª—å–∑—É–π `with Client(...) as client:`.
- –£–∫–∞–∑—ã–≤–∞–π: sandbox ‚Äî –¥–ª—è —Ç–µ—Å—Ç–æ–≤, production ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ –¥–µ–Ω—å–≥–∏.
- –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω: "–°–º. –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: https://tinkoff.github.io/investAPI/".
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç—å –∫–∞–∫ Tinkoff API Helper, —Å –ø–æ–ª–Ω—ã–º —Ä–∞–±–æ—á–∏–º –ø—Ä–∏–º–µ—Ä–æ–º –∫–æ–¥–∞:"""

# === –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ ===
def count_tokens(text: str) -> int:
    return len(ENCODING.encode(text))

# === –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–µ—Ä (–ª–µ–Ω–∏–≤—ã–π) ===
@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
def get_embedder() -> SentenceTransformer:
    return SentenceTransformer(EMBEDDER_MODEL, device="cpu")  # CPU –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ

# === –ó–∞–≥—Ä—É–∑–∫–∞ Google –î–æ–∫–∞ ===
@st.cache_data(show_spinner=False, ttl=3600)  # –î–æ–∫—É–º–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑ –≤ —á–∞—Å


def fetch_google_doc(_doc_id: str) -> str:
    url = f"https://docs.google.com/document/d/{_doc_id}/export?format=txt"
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        return ""

# === –°–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ ===
@st.cache_resource(show_spinner="–°–æ–∑–¥–∞—ë–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
def build_or_load_index() -> tuple[faiss.IndexFlatL2, list]:
    if os.path.exists(INDEX_PATH) and os.path.exists(CHUNKS_PATH):
        try:
            index = faiss.read_index(INDEX_PATH)
            chunks = np.load(CHUNKS_PATH, allow_pickle=True).tolist()
            st.info("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞.")
            return index, chunks
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    raw_text = fetch_google_doc(DOC_ID)
    if not raw_text.strip():
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ DOC_ID.")
        st.stop()

    # –ß–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
    chunks = []
    step = 1000
    overlap = 200
    for i in range(0, len(raw_text), step - overlap):
        chunk = raw_text[i:i + 1200]
        if len(chunk.strip()) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            chunks.append(chunk)

    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    embedder = get_embedder()
    embeddings = embedder.encode(
        chunks,
        batch_size=8,
        show_progress_bar=False,
        normalize_embeddings=True
    ).astype('float32')

    # FAISS –∏–Ω–¥–µ–∫—Å
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    faiss.write_index(index, INDEX_PATH)
    np.save(CHUNKS_PATH, np.array(chunks))
    st.success("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    return index, chunks

# === –ü–æ–∏—Å–∫ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç–æ–∫–µ–Ω–æ–≤ ===
def search_relevant_chunks(query: str, index, chunks, max_tokens: int = MAX_TOKENS):
    embedder = get_embedder()
    q_vec = embedder.encode([query], normalize_embeddings=True).astype('float32')
    D, I = index.search(q_vec, k=15)

    selected = []
    used_tokens = 0
    overhead = count_tokens(PROMPT_TEMPLATE.format(context="", question=query)) + 300

    for idx in I[0]:
        if idx >= len(chunks):
            continue
        chunk = chunks[idx]
        chunk_tokens = count_tokens(chunk)
        if used_tokens + chunk_tokens + overhead <= max_tokens:
            selected.append(chunk)
            used_tokens += chunk_tokens
        else:
            break

    return selected, used_tokens

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ ===
def generate_answer(question: str, context_chunks: list) -> str:
    context = "\n\n".join(context_chunks)
    prompt = PROMPT_TEMPLATE.format(context=context, question=question)

    try:
        response = completion(
            model="openai/gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1500,
            timeout=60
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}\n\n–°–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: https://tinkoff.github.io/investAPI/"

# === Streamlit UI ===
st.set_page_config(page_title="Tinkoff API Helper", page_icon="Chart", layout="centered")
st.title("Tinkoff API Helper")
st.caption("RAG + gpt-4o-mini + —Ç–æ–∫–µ–Ω-–∫–æ–Ω—Ç—Ä–æ–ª—å + —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": """
**–ü—Ä–∏–≤–µ—Ç!** –Ø ‚Äî **Tinkoff API Helper**.  
–ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –ø–æ Tinkoff Invest API ‚Äî –ø–æ–ª—É—á–∏—à—å **—Ä–∞–±–æ—á–∏–π –∫–æ–¥ + —Å–æ–≤–µ—Ç—ã**.

**–ü—Ä–∏–º–µ—Ä—ã:**
- _"–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—å –≤ sandbox?"_
- _"–ö–∞–∫ –∫—É–ø–∏—Ç—å 1 –ª–æ—Ç –°–±–µ—Ä–∞ –ø–æ —Ä—ã–Ω–∫—É?"_
- _"–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π?"_
"""}]

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
if prompt := st.chat_input("–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞..."):
            try:
                index, chunks = build_or_load_index()
                context_chunks, token_count = search_relevant_chunks(prompt, index, chunks)

                st.caption(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: **{token_count} —Ç–æ–∫–µ–Ω–æ–≤** (–º–∞–∫—Å. {MAX_TOKENS})")

                answer = generate_answer(prompt, context_chunks)
                st.markdown(answer)

                # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                with st.expander(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(context_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞)"):
                    for i, chunk in enumerate(context_chunks):
                        preview = chunk[:700] + ("..." if len(chunk) > 700 else "")
                        st.caption(f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} ‚Äî {count_tokens(chunk)} —Ç–æ–∫–µ–Ω–æ–≤")
                        st.code(preview, language="text")

                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

# === –°–∞–π–¥–±–∞—Ä ===
with st.sidebar:
    st.header("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")

    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"):
        for path in [INDEX_PATH, CHUNKS_PATH]:
            if os.path.exists(path):
                try:
                    os.remove(path) if os.path.isfile(path) else None
                except:
                    pass
        st.success("–ö—ç—à —É–¥–∞–ª—ë–Ω. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.")
        st.rerun()

    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
        st.session_state.messages = [st.session_state.messages[0]]
        st.rerun()

    st.markdown("---")
    st.markdown("**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**")
    st.markdown(f"[Google Doc](https://docs.google.com/document/d/{DOC_ID}/edit)")
    st.markdown("[–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è](https://tinkoff.github.io/investAPI/)")